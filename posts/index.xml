<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on あむ&#39;s TechBlog</title>
    <link>https://techblog.gadget-amphone.com/posts/</link>
    <description>Recent content in Posts on あむ&#39;s TechBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>jp-JA</language>
    <lastBuildDate>Sat, 13 Jan 2024 23:48:31 +0900</lastBuildDate>
    <atom:link href="https://techblog.gadget-amphone.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rustの所有権について学習をしてみる</title>
      <link>https://techblog.gadget-amphone.com/posts/rust-ownership/</link>
      <pubDate>Sat, 13 Jan 2024 23:48:31 +0900</pubDate>
      <guid>https://techblog.gadget-amphone.com/posts/rust-ownership/</guid>
      <description>はじめに 過去に少しだけRustの学習をしていました。今回改めてRustの学習を始めるにあたり、Rustの所有権について学習をしていきます。&#xA;所有権とは Rustの所有権とは、メモリの安全性を保証するための概念です。Rustの所有権には以下の3つのルールがあります。&#xA;各値は、その値の所有者という変数を持っています。 一度に一つの変数のみが値の所有者であることができます。 所有者がスコープから外れると、値は破棄されます。 なるほど、日本語としては理解しましたが、実際にコードを書いてみないと理解できない部分もあると思います。そこでコードを書いていきます。&#xA;所有権の移動 一度に一つの変数のみが値の所有者であることができます。 の部分について確認をしていきます。下記は変数s1をs2に代入したコードです。&#xA;fn main() { let s1 = String::from(&amp;#34;hello&amp;#34;); let s2 = s1; println!(&amp;#34;{}, world!&amp;#34;, s1); // これはエラーになる } ビルドすると下記の通りエラーが出ました。これは文字列Helloという値の所有権がs1→s2となったためエラーとなるようです。&#xA;error[E0382]: borrow of moved value: `s1` --&amp;gt; src/main.rs:5:28 | 2 | let s1 = String::from(&amp;#34;hello&amp;#34;); | -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait 3 | let s2 = s1; | -- value moved here 4 | 5 | println!</description>
    </item>
    <item>
      <title>ブログ内コードブロックの表示がおかしい問題を治した話</title>
      <link>https://techblog.gadget-amphone.com/posts/ios-webkit-codeblock/</link>
      <pubDate>Fri, 12 Jan 2024 01:27:43 +0900</pubDate>
      <guid>https://techblog.gadget-amphone.com/posts/ios-webkit-codeblock/</guid>
      <description>はじめに ふと弊サイトをiPhoneで確認すると下記の通り、コードブロック内が行単位でフォントサイズがめちゃくちゃになってました。&#xA;そういえばSafariでテストしてなかったと思い見たらコードブロック終わってて草 pic.twitter.com/2ojFjT6YCu&#xA;&amp;mdash; あむ (@gadget_amphone) January 11, 2024 今回はこれを修正した話をまとめます。&#xA;解決方法 先に結論を書くと、コードブロックのフォントサイズを指定するCSS部分に下記を追加して解決しました。&#xA;code span { font-size: 0.8rem; -webkit-text-size-adjust: none; /* 追加部分、WebKit特有のフォントサイズ調整機能を無効化 */ } 解決までの道のり 当初はCSSの設定が誤っているのだと思い込んでいました。しかし、WindowsやMac、Androidのブラウザでは問題なく表示されていたため、原因はOSやブラウザの違いにあるのではないかと考えました。&#xA;はじめにiOS Simulatorの開発者ツールで分析を行いました。その結果、コードブロック内の各行は並列で同じタグの要素が並んでおり、全てに同じCSSが適用されていました。にも関わらず、何故かフォントサイズが異なっていることがわかりました。&#xA;これはあまりにも不可解でしたが、iOS限定の事象であり、Webkit特有の問題があるのではないかと考えました。そこで今回の設定でWebkit特有のフォントサイズ調整機能を無効化することで解決にいたりました。&#xA;まとめ 普段はWebのフロント開発をしていないため、このような事象は初体験となりました。普段の技術学習の中で、iOSのブラウザはWebkitを全て使っており、時々変な挙動をすることは頭の片隅にあったので今回は何とか解決することができました。同じ現象にあたった方がいらっしゃいましたら、是非こちらを活用してください。</description>
    </item>
    <item>
      <title>llama-cpp-pythonとFlaskでLLMサーバをたててみた</title>
      <link>https://techblog.gadget-amphone.com/posts/llm-flask/</link>
      <pubDate>Wed, 10 Jan 2024 21:30:41 +0900</pubDate>
      <guid>https://techblog.gadget-amphone.com/posts/llm-flask/</guid>
      <description>はじめに 今回は、llama-cpp-pythonとFlaskを使って、LLMサーバをたててみたので、その手順を紹介します。特にStreamデータとして返却する部分に苦労したため、自分の備忘録としても残しておきます&#xA;事前準備 事前準備に下記をインストールします&#xA;Flaskのインストール llama-cpp-pythonのインストール 学習済み大規模言語モデルのダウンロード Flaskのインストール FlaskはPythonでWebアプリケーションを作成するための軽量なフレームワークです。今回はこちらを使って、ローカル環境にLLMサーバをたてるためインストールしていきます&#xA;Flaskのインストールは下記のコマンドを使います&#xA;pip install Flask llama-cpp-pythonのインストール llama-cpp-pythonは、Llama2のPythonラッパーです。Llama2はLarge Language Model(大規模言語モデル)の一種類です。こちらを使うと簡単にローカル環境でLLMをたてることができます。ローカル環境にLLMサーバをたてるためインストールしていきます&#xA;pip install llama-cpp-python 学習済み大規模言語モデルのダウンロード 大規模言語モデルを個人の環境で学習するのはとても困難です。そこで他の方が公開している学習済みモデルを使います。そこで便利なのがHugging Faceという学習済みモデル公開サイトです。今回はこの中でELYZA社のモデルをggufファイル化してくださっていた方がいたので利用させていただきます。&#xA;こちらにアクセスし、ELYZA-japanese-Llama-2-7b-instruct-q8_0.ggufをダウンロードしてください。&#xA;Pythonのコードを書く 先に完成系を下記に示します。&#xA;from llama_cpp import Llama from flask import Flask, request, Response llm = Llama(model_path=&amp;#34;./ELYZA-japanese-Llama-2-7b-instruct-q8_0.gguf&amp;#34;, chat_format=&amp;#34;llama-2&amp;#34;) app = Flask(__name__) @app.route(&amp;#39;/llama&amp;#39;, methods=[&amp;#39;POST&amp;#39;]) def llama_chat(): data = request.json message = data[&amp;#39;message&amp;#39;] def generate(): response = llm.create_chat_completion(messages=[{&amp;#39;role&amp;#39;: &amp;#39;user&amp;#39;, &amp;#39;content&amp;#39;: message}], stream=True, stop=None) for chunk in response: if &amp;#39;content&amp;#39; in chunk[&amp;#39;choices&amp;#39;][0][&amp;#39;delta&amp;#39;]: yield chunk[&amp;#39;choices&amp;#39;][0][&amp;#39;delta&amp;#39;][&amp;#39;content&amp;#39;] return Response(generate(), mimetype=&amp;#39;text/plain&amp;#39;) if __name__ == &amp;#34;__main__&amp;#34;: app.</description>
    </item>
  </channel>
</rss>
